{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wydialang/reviewclassification/blob/master/Copy_of_Yelp_Review_Sentiment_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svLhCiU3Evm6"
      },
      "source": [
        "# Introduction to Yelp Review Sentiment Classification\n",
        "\n",
        "In this project, we will build a classifier that can predict a user's rating of a given restaurant from their review. In doing so, we will explore the broader topic of sentiment analysis, that is, being able to quantify an individual's opinion about a particular topic merely from the words they use. Nowadays, sentiment analysis is used widely by companies in order to better understand their user's preferences and tastes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZgELVS8I6kc"
      },
      "source": [
        "![Example of a Yelp review](https://wordstream-files-prod.s3.amazonaws.com/s3fs-public/styles/simple_image/public/images/yelp-reviews-filtered.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jS5ThMCEvnC",
        "cellView": "form"
      },
      "source": [
        "#@title Import our libraries (this may take a minute or two)\n",
        "import pandas as pd   # Great for tables (google spreadsheets, microsoft excel, csv). \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import nltk\n",
        "import spacy\n",
        "import wordcloud\n",
        "import os # Good for navigating your computer's files \n",
        "import sys\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "!python -m spacy download en_core_web_md\n",
        "import en_core_web_md\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsF8tioZLicU"
      },
      "source": [
        "#@title Import our data\n",
        "\n",
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?id=1u0tnEF2Q1a7H_gUEH-ZB3ATx02w8dF4p', 'yelp_final.csv', True)\n",
        "data_file  = 'yelp_final.csv'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ267zCBOjet"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BLs_2JkEvnw"
      },
      "source": [
        "First we read in the file containing the reviews and take a look at the data available to us."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dZ_lymcN_K9"
      },
      "source": [
        "# read our data in using 'pd.read_csv('file')'\n",
        "yelp = pd.read_csv(data_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp0c1vAdEvoF"
      },
      "source": [
        "#@title Show data\n",
        "yelp.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjL5FrSLEvoP"
      },
      "source": [
        "We have access to 7 columns of data. For our purposes the business_id and user_id information are not important. Also as you can see the data in these columns do not really correspond to business names or user names. They are random strings, generated by a process called hashing to anonymize the users and businesses.\n",
        "\n",
        "What is *hashing*? We do not need to go in depth about this as it is a concept from computer science unrelated to machine learning. It is a way of *encrypting* data using some predefined function, that maps any kind of data to a random string of a defined length. You can read more about it [here](https://medium.com/tech-tales/what-is-hashing-6edba0ebfa67). \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB1yKcUtcpg9"
      },
      "source": [
        "#@title **Run to remove unnecessary columns** { display-mode: \"form\" }\n",
        "yelp.drop(labels=['business_id','user_id'],inplace=True,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MExj8roOEvog"
      },
      "source": [
        "The text column is the one we are primarily focused with. Let's take a look at a few of these reviews to better understand our problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la3rUPKgEvoi",
        "cellView": "form"
      },
      "source": [
        "#@title Check the text in differently rated reviews\n",
        "num_stars =  1#@param {type:\"integer\"}\n",
        "\n",
        "for t in yelp[yelp['stars'] == num_stars]['text'].head(20).values:\n",
        "    print (t) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GZ6NN4yEvos"
      },
      "source": [
        "We can start to see that there are certain quantitative differences between highly rated reviews and poorly rated reviews. Certain words, for example, 'delightful', 'impressive', 'amazing', might be more associated with 4 or 5 star reviews. However one might be able to see that these words might also be present in a 2 star review. For example: \"The seating and ambience were impressive, but the food served to us was not\". \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpU7nrWTEvov"
      },
      "source": [
        "### **Exercise 1**\n",
        "\n",
        "Can you think of any combinations of words, or rules, that would indicate how many stars a given review corresponds to? Note them down below:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxuZyKOKy6Cc",
        "cellView": "form"
      },
      "source": [
        "#@title Rules\n",
        "rule_1 = \"bad\" #@param {type:\"string\"}\n",
        "rule_2 = \"\" #@param {type:\"string\"}\n",
        "rule_3 = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4PQg8FhEvow"
      },
      "source": [
        "It is not really the presence of individual words that gives us an indication of the stars given to a review, but more  the *relative occurrence* of these words in each review that might give us an indication of a user's rating. If this does not make sense immediately, do not worry, we will come back to this concept later. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQHod14KEvoz"
      },
      "source": [
        "#### Word Clouds\n",
        "\n",
        "Another way to take a look at the most prominent words in any given star rating is through the use of word clouds. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHY5IhnKEvo8"
      },
      "source": [
        "Edit the value in the cell below to see the word cloud for each star rating."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtMnf1zLEvo_"
      },
      "source": [
        "#@title Word cloud for differently rated reviews\n",
        "num_stars =  1#@param {type:\"integer\"}\n",
        "this_star_text = ''\n",
        "for t in yelp[yelp['stars'] == num_stars]['text'].values: # form field cell\n",
        "    this_star_text += t + ' '\n",
        "    \n",
        "wordcloud = WordCloud()    \n",
        "wordcloud.generate_from_text(this_star_text)\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn4v3upxEvpL"
      },
      "source": [
        "**What are the differences between the reviews that have 1, 2, 3, 4, and 5 stars?**\n",
        "\n",
        "*As* we can see, in this case, the word cloud does not give us a great deal of distinguishing information between reviews that have 1, 2, 3, 4, or 5 stars. All these reviews seem to prominently feature words such as 'place', 'food', 'service' and 'table'. Human intuition will only get us so far. \n",
        "\n",
        "Before we go any further, we will need to clean up our text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8raBKzTEvpM"
      },
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pby7LlwhEvpN"
      },
      "source": [
        "#### Tokenization\n",
        "\n",
        "First of all, we would like to convert each review from a single string into a list of words (this is a process known as tokenizaton). All NLP algorithms require a list of words as arguments and not actual sentences. Enter some example text into the cell below to see the tokenized version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XOaa1uEEvpY"
      },
      "source": [
        "#@title Basic tokenization example\n",
        "example_text = \"All the people I spoke to were super nice and very welcoming.\" #@param {type:\"string\"}\n",
        "tokens = word_tokenize(example_text)\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKCP5Q_LEvpg"
      },
      "source": [
        "#### Stopwords\n",
        "\n",
        "We can see that certain particular words might be associated with 4 or 5 star reviews, and some words would be associated with 1 or 2 star reviews. However, at the same time, there are some words that do not really possess any relevant information for our current problem. In the field of NLP there is a concept of words that are \"stopwords\" - words that exist to provide grammatical structure, but do not convey information about the particular subject. Edit the cell below to see if a given word is a stop word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nqq4w-ZrEvpj"
      },
      "source": [
        "#@title Check if a word is a stop word\n",
        "example_word = \"the\" #@param {type:'string'}\n",
        "if example_word.lower() in STOP_WORDS:\n",
        "  print (example_word + \" is a stop word.\")\n",
        "else:\n",
        "  print (example_word + \" is NOT a stop word.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vubl4ejEvpv"
      },
      "source": [
        "We would like to remove these stopwords from the user reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUZ35Ay9Evpy"
      },
      "source": [
        "Tokenization and removal of stop words are universal to nearly every NLP application. In some cases, additional cleaning may be required (for example, removal of proper nouns, removal of digits) but we can build a text preprocessing function with these \"base\" cleaning steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPsaFxJiEvp9"
      },
      "source": [
        "Putting all these together, we can come up with a text cleaning function that we can apply to all of our reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mLW7krRcBIG"
      },
      "source": [
        "# Intro to Spacy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcX6gwcFaFRd"
      },
      "source": [
        "Now a library we are going to use is Spacy.\n",
        "\n",
        "Spacy, like the other Python libraries, gives us a lot of useful functions that we can directly use.\n",
        "\n",
        "Unlike the other libraries, spacy does a lot more. It gives us insights into the English Language (actually, you can use many other languages here). The way it does is it downloads a set of information about the English Language and lets you use it.\n",
        "\n",
        "`\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "`\n",
        "\n",
        "Now when you call any sentence with this nlp object you will get quite a few helper functions that are really useful for a lot of tasks.\n",
        "\n",
        "Lets explore some of the things that Spacy provides.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldEPkz8NcI6_"
      },
      "source": [
        "nlp = en_core_web_md.load()\n",
        "doc = nlp(u\"We are running out of time! Are we though?\")\n",
        "doc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWyXczXkePiI"
      },
      "source": [
        "The doc object has a lot of nice properties. For instance you can get the text of each of the words and the length of each of the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQifXTc_DYK7"
      },
      "source": [
        "doc = nlp(u\"We are running out of time! Are we though?\")\n",
        "token = doc[0] # Get the first word in the text.\n",
        "assert token.text == u\"We\" # Check that the token text is 'We'.\n",
        "assert len(token) == 2 # Check that the length of the token is 2."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUI5yFeoekLQ"
      },
      "source": [
        "It has some word vectors that we can use. Though note that it doesn't have all the words. Let's import a new dataset of word (this may take a minute or so):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-yAoiZLLjbl"
      },
      "source": [
        "We can get the word embedding of a particular word in our document as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0tBmojQeNhn"
      },
      "source": [
        "doc = nlp(u\"I like apples\")\n",
        "print(doc)\n",
        "appleVariable = doc[2]\n",
        "\n",
        "print(appleVariable.vector) # Each word is being represented by 300 dimensional vector embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNPzJouoJj3f"
      },
      "source": [
        "The word 'Apple' is represented by 300 dimensional vector embedding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmX2ENExhwj2"
      },
      "source": [
        "### Fun things you can do with word vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBagECaVMQXZ"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "You can get the similarity of two words via the following method:\n",
        "\n",
        "\n",
        "```\n",
        "doc = nlp(u\"keyboard and mouse\")\n",
        "word1 = doc[0]\n",
        "word2 = doc[2]\n",
        "word1.similarity(word2)\n",
        "```\n",
        "\n",
        "**Use the above methodology to find two words with a similarity greater than 0.77 and two words with similarity less than 0.15. Share both of your word pairings with the class.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD8fVPwlM4Bn"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "doc = nlp(u\"bird and disapperance\")\n",
        "word1 = doc[0]\n",
        "word2 = doc[2]\n",
        "word1.similarity(word2)\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bwasn11BfD4"
      },
      "source": [
        "As we saw before, the language in 4 star reviews is quite similar to the language in 5 star reviews. So the text in those reviews might not be very useful and we can drop those rows from our data.\n",
        "\n",
        "Although the text in the 3 star reviews is not very similar to the 1 or 2 star reviews, it is quite different from the language used in the 5 star reviews. So we could actually group those reviews together with the 1 and 2 star reviews.\n",
        "\n",
        "In order to reduce our problem to a **binary classification** problem, we will:\n",
        "\n",
        " - remove all 4 star reviews\n",
        " - label 5 star reviews as 'good'\n",
        " - label 1, 2, 3 star reviews as 'bad'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn__oGIOEvrA"
      },
      "source": [
        "Run the cell below to get rid of 4 star reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrgQp7gEEvrC"
      },
      "source": [
        "yelp = yelp[yelp.stars != 4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNgLAFNpEvrG"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "Complete the second line of code in the cell below, and run it to re-categorize our reviews. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck4iX6PITzHS"
      },
      "source": [
        "def is_good_review(stars):\n",
        "    if stars > 3: ### TODO: FILL IN THE IF STATEMENT HERE ###\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Change the stars field to either be 'good' or 'bad'.\n",
        "yelp['is_good_review'] = yelp['stars'].apply(is_good_review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfIUrRtEWr3H"
      },
      "source": [
        "## One-Hot Vectors\n",
        "\n",
        "How do we convert our text to numbers in a structured way that we can feed into a machine learning algorithm? One way to do it is to use a concept called \"one-hot encoding\". We can see this concept with the following example. Suppose we have a sentence \"great tacos at this restaurant\". Its one-hot encoding would be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucOjGk7qXjbb",
        "cellView": "form"
      },
      "source": [
        "#@title Run this to see the one-hot encoding of 'great tacos at this restaurant'\n",
        "print('{:^5}|{:^5}|{:^4}|{:^4}|{:^10}'.format('great', 'tacos', 'at','this','restaurant'))\n",
        "print('--------------------------------------------')\n",
        "print('{:^5}|{:^5}|{:^4}|{:^4}|{:^10}'.format('1', '0', '0','0','0'))\n",
        "print('{:^5}|{:^5}|{:^4}|{:^4}|{:^10}'.format('0', '1', '0','0','0'))\n",
        "print('{:^5}|{:^5}|{:^4}|{:^4}|{:^10}'.format('0', '0', '1','0','0'))\n",
        "print('{:^5}|{:^5}|{:^4}|{:^4}|{:^10}'.format('0', '0', '0','1','0'))\n",
        "print('{:^5}|{:^5}|{:^4}|{:^4}|{:^10}'.format('0', '0', '0','0','1'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOPTTwTqAfDN"
      },
      "source": [
        "###Exercise: One Hot Encoding - Sentences\n",
        "\n",
        "Let's say you just have two reviews(vocabulary) as following:\n",
        "\n",
        "1. I loved the restaurant\n",
        "2. I hate the food \n",
        "\n",
        "The vocabulary would consist of unique words across both the reviews. Now create one-hot encoded vector for reviews above. (No coding needed)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0-l-AkEBaXh",
        "cellView": "form"
      },
      "source": [
        "#@title Solution\n",
        "\n",
        "#Unique Vocab - ['I','loved','the','restaurant','hate','food']\n",
        "#One Hot Encoding - 1st - [1,1,1,1,0,0] , 2nd - [1,0,1,0,1,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpZ11S8lEvrM"
      },
      "source": [
        "## Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43hbR0vha7E3"
      },
      "source": [
        "Building upon the concept of one-hot encoding is the **bag of words** model. If one-hot encoding is a way to represent individual words as vectors, then you can think of bag of words as a way to represent sentences (or larger pieces of text) as the **sum** of the one-hot encoding vectors of each of the words. Let's explain with an example. \n",
        "\n",
        "Suppose we want to represent the review: \n",
        "**\"The food was great. The ambience was also great.\"** as a bag of words.\n",
        "\n",
        "First we define our vocabulary. This is *each unique word* in the review. So our vocabulary is **[the, food, was, great, ambience, also]**.\n",
        "\n",
        "What are our one hot encodings? \n",
        "\n",
        "the = (1,0,0,0,0,0)\n",
        "\n",
        "food = (0,1,0,0,0,0)\n",
        "\n",
        "was = (0,0,1,0,0,0)\n",
        "\n",
        "great = (0,0,0,1,0,0)\n",
        "\n",
        "ambience = (0,0,0,0,1,0)\n",
        "\n",
        "also = (0,0,0,0,0,1).\n",
        "\n",
        "So far, so simple. Now how do we represent the review we mentioned above as a bag of words? We know we only have 6 words in our vocabulary, so our bag of words vector will also only be 6 elements long. To construct it, we can start off with a (0,0,0,0,0,0) vector, and then pass through each word in the review. For each word we encounter, we simply add its one hot encoding to our vector! So for our review, the bag of words representation will be\n",
        "\n",
        "**(2,1,2,2,1,1)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbFU78B5bXka"
      },
      "source": [
        "## Creating our Bag of Words\n",
        "\n",
        "Back to our data. We want to select the features for our model and the output classes from our data. What are the features? We are only using the review text to make predictions for our model. And the output classes are the 'good' and 'bad' review classes we created just above. \n",
        "\n",
        "By convention, we represent our entire set of features as X, and our target output as y. Running the cell below will create the relevant X and y for our problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t6HQm1vEvrQ"
      },
      "source": [
        "X = yelp['text']\n",
        "y = yelp['is_good_review']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhRyg_YEeA5t"
      },
      "source": [
        "Running the cell below will create an object we can use to *transform* each piece of raw text into a bag of words vector.\n",
        "CountVectorizer is a useful class we can call from scikit-learn that will help us create this object. It even has a helpful parameter that we can set to our tokenize function to preprocess the raw text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrSQAeKjAiXJ"
      },
      "source": [
        "#@title Initialize the text cleaning function { display-mode: \"form\" }\n",
        "def tokenize(text):\n",
        "    clean_tokens = []\n",
        "    for token in nlp(text):\n",
        "        if (not token.is_stop) & (token.lemma_ != '-PRON-') & (not token.is_punct): # -PRON- is a special all inclusive \"lemma\" spaCy uses for any pronoun, we want to exclude these \n",
        "            clean_tokens.append(token.lemma_)\n",
        "    return clean_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blZ7RJ2zEvrU"
      },
      "source": [
        "bow_transformer = CountVectorizer(analyzer=tokenize, max_features=1600).fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaheGj_RmKW7"
      },
      "source": [
        "We can see our entire vocabulary by running the cell below! You will also notice an index associated with each word - this is the position of each word in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TjdgYVxmKgd"
      },
      "source": [
        "bow_transformer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGEf8h1lEvrY"
      },
      "source": [
        "We can see the length of the vocabulary stored in the transformer object by running the cell below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upN1gxm5Evrb"
      },
      "source": [
        "len(bow_transformer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsFa7Nu5Evr4"
      },
      "source": [
        "Finally, to finish preparing our data, we can use the transformer to transform our entire training set (X) into a series of bag of words vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRJJe2HGEvr6"
      },
      "source": [
        "X = bow_transformer.transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8AUxyLHEvr_"
      },
      "source": [
        "## Training a Baseline Classification Model (Logistic Regression)\n",
        "\n",
        "Our classification problem is a classic two-class classification problem, and so we will use the tried and tested **Logistic Regression** machine learning model from yesterday's class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIpbdNZwgRTn"
      },
      "source": [
        "# import the logistic regression model from scikit-learn\n",
        "logistic_model = LogisticRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q459D2jwaUCI"
      },
      "source": [
        "We will use 20% of our data as test data. If you run the cell below, it will randomly split the data such that 80% of it is training data and 20% of it is data we can use to test the predictions from our trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PThy6pNUEvsA"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8K4C0iQEvsE"
      },
      "source": [
        "### **Exercise 4**\n",
        "Now that we have our model selected and our data split, let's train our model. Refer to yesterday's logistic regression notebook for a remainder about how to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip5a0e4sdB4Q"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "logistic_model.fit(X_train, y_train)\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApCzou0tEvsL"
      },
      "source": [
        "## Exercise 5\n",
        "\n",
        "Once the model is trained, we can generate predictions from our test data. Just like the fit() function above, there is a similar predict() function that we can use once our model is trained. Create your model's predictions on the text model. Next, use the true positive (TP), false positive (FP), true negative (TN), and false negative (FN) rates to evaluate the accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omlRkPWNf95J"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "# Get our predictions.\n",
        "preds = logistic_model.predict(X_test)\n",
        "\n",
        "# Get the confusion matrix.\n",
        "cm = confusion_matrix(y_test, preds)\n",
        "\n",
        "# Get TP, FP, TN, and FN rates.\n",
        "TP = cm[0][0]\n",
        "TN = cm[1][1]\n",
        "FP = cm[0][1]\n",
        "FN = cm[1][0]\n",
        "\n",
        "# Calculate and print accuracy.\n",
        "accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
        "print (\"The accuracy of the model is \" + str(accuracy*100) + \"%\")\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKgDnIcaEvsX"
      },
      "source": [
        "Not perfect, but definitely better than we would have expected at random (50%).\n",
        "\n",
        "Enter an example review to see if our model predicts it as a positive one or a negative one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euuR1VWWEvsX"
      },
      "source": [
        "#@title Enter an example review, and see if it is classified as good or bad\n",
        "example_review = \"good!!!!!!!!!!!!!\" #@param {type:'string'}\n",
        "prediction = logistic_model.predict(bow_transformer.transform([example_review]))\n",
        "\n",
        "if prediction:\n",
        "  print (\"This was a GOOD review!\")\n",
        "else:\n",
        "  print (\"This was a BAD review!\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV0Fe3GpHd2_"
      },
      "source": [
        "###Exercise 6\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1i_LdIeHqgo"
      },
      "source": [
        "Now change the max features attribute while creating your bag of words model. Discuss how the change affects the accuracy of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k-2NwiLHL4w"
      },
      "source": [
        "###Exercise 7 (Optional)\n",
        "We used Logistic regression for our baseline model. However we could also use a separate model called Multinomial Naive Bayes to perform our classification.\n",
        "Naive Bayes uses Bayes' Theorem of probability to predict class of new input data. The important assumption Naive Bayes makes is that the presence of one feature is independent of the presence of any other.\n",
        "\n",
        "Let's build a model using a Naive Bayes classifier!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPg0Y7cjHH2c"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "nb_model = MultinomialNB()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nqtNo_qIsbh"
      },
      "source": [
        "We can train and generate predictions from this model in the same way we did for our Logistic Regression model. Try training this model on the same data and see if it performs better or worse than our logistic regression model. Then, evaluate the model accuracy as your did for the Logistic Regression classifier.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q-UOCh0I0BR"
      },
      "source": [
        "###YOUR CODE HERE####\n",
        "nb_model.fit(X_train, y_train)\n",
        "nb_preds = nb_model.predict(X_test) \n",
        "nbcm = confusion_matrix(y_test, nb_preds)\n",
        "accuracy = (nbcm[0][0] + nbcm[1][1])/(nbcm[0][0] + nbcm[0][1] + nbcm[1][0]+ nbcm[1][1])\n",
        "print (\"The accuracy of the model is \" + str(accuracy*100) + \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TffS4NT-BSS"
      },
      "source": [
        "Experiment with the models you've learned so far and try to get the highest accuracy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vge8vYO5Evq1"
      },
      "source": [
        "### Exercise 8\n",
        "\n",
        "We saw earlier that our WordClouds didn't give us too much useful information. What words were present across all kinds of reviews? If we take those words out of the reviews, we could possibly have more useful WordClouds.\n",
        "\n",
        "Identify some of these common words and add them to the list in the cell below. Then, run the word cloud to see the words that show up across review types.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drOWMCAQFtG7"
      },
      "source": [
        "###Fill in the list with the words you think which should be removed. Ex: ['food','time','service']###\n",
        "common_words = [\"food\", \"time\", \"good\", \"place\", \"one\", \"us\", \"even\", \"will\", \"jt\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCO4a54tkl8j",
        "cellView": "form"
      },
      "source": [
        "#@title Solution\n",
        "common_words = ['got','will','go','even','food','service',\n",
        "                 'place','restuarant','good','one','really',\n",
        "                 'time','back','review','customer','order']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR9E844brnHu"
      },
      "source": [
        "#@title Word cloud for differently rated reviews\n",
        "num_stars =  2#@param {type:\"integer\"}\n",
        "this_star_text = ''\n",
        "for t in yelp[yelp['stars'] == num_stars]['text'].values: # form field cell\n",
        "    this_star_text += t + ' '\n",
        "    for word in common_words:\n",
        "      this_star_text = this_star_text.replace(word,'')\n",
        "    \n",
        "wordcloud = WordCloud()    \n",
        "wordcloud.generate_from_text(this_star_text)\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFeI61HGY2Kt"
      },
      "source": [
        "### (Optional Advanced Exercise) Training Logistic Regression with Word Vectors from Spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9DUXQXSaMPU"
      },
      "source": [
        "We can use the word vectors we introduced earlier to get more sophisticated representations of our Yelp reviews. First, we get the text data from our dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs6fasZuOlZc"
      },
      "source": [
        "X_data = yelp['text']\n",
        "y_data = yelp['is_good_review']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_n7uJ20a_yH"
      },
      "source": [
        "This helper function uses the Spacy `nlp` object to remove stop words, pronouns, and punctuation. It is identical to the `tokenize` function above, except it preserves the non-string attributes of the clean tokens (i.e. `token.vector`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAOs86fEQ_2l"
      },
      "source": [
        "def tokenize_vecs(text):\n",
        "    clean_tokens = []\n",
        "    for token in nlp(text):\n",
        "        if (not token.is_stop) & (token.lemma_ != '-PRON-') & (not token.is_punct): \n",
        "          # -PRON- is a special all inclusive \"lemma\" spaCy uses for any pronoun, we want to exclude these \n",
        "            clean_tokens.append(token)\n",
        "    return clean_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0XqDYs_b4V8"
      },
      "source": [
        "We want to represent each Yelp review with a vector. Since each review consists of multiple words, we want to find a way to create one vector for each review. \n",
        "\n",
        "Would adding the word vectors work? What about averaging? Which would be preferrable?\n",
        "\n",
        "Implement your solution below: convert our array of reviews into an array of vector representations of those reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tKKKLymMNsd"
      },
      "source": [
        "X_word2vec = []\n",
        "for text in X_data:\n",
        "  review = tokenize_vecs(text) # returns cleaned list of spacy tokens\n",
        "  #### YOUR CODE HERE\n",
        "  review_vec = 0\n",
        "  for word in review:\n",
        "    review_vec += word.vector\n",
        "  review_vec = review_vec / len(review)\n",
        "  X_word2vec.append(review_vec)\n",
        "\n",
        "  \n",
        "  #### END CODE\n",
        "  \n",
        "X_word2vec = np.array(X_word2vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frxz5RO5c0ft"
      },
      "source": [
        "Now we follow the same procedure for training and testing the logistic regression that we used for the Bag of Words data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNgGYuGdOt9u"
      },
      "source": [
        "# import a fresh logistic regression model from scikit-learn\n",
        "logistic_model = LogisticRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lusHdcWSfEB"
      },
      "source": [
        "# train-test split\n",
        "X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(X_word2vec, y, test_size=0.2, random_state=101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTPDGo_FSJeV"
      },
      "source": [
        "logistic_model.fit(X_train_word2vec, y_train_word2vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmOilBMVStA5"
      },
      "source": [
        "#@title Run this to print the accuracy for our word2vec classifier\n",
        "\n",
        "\n",
        "# Get our predictions.\n",
        "preds = logistic_model.predict(X_test_word2vec)\n",
        "\n",
        "# Get the confusion matrix.\n",
        "cm = confusion_matrix(y_test_word2vec, preds)\n",
        "\n",
        "# Get TP, FP, TN, and FN rates.\n",
        "TP = cm[0][0]\n",
        "TN = cm[1][1]\n",
        "FP = cm[0][1]\n",
        "FN = cm[1][0]\n",
        "\n",
        "# Calculate and print accuracy.\n",
        "accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
        "print (\"The accuracy of the model is \" + str(accuracy*100) + \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywQgiPef_0dN"
      },
      "source": [
        "For Instructors: Target Accuracy = 76.15384615384615%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcqbg3NSNriB"
      },
      "source": [
        "### Challenge Exercise: Calculating Similarity and Analogies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTyeFBh0CbNO"
      },
      "source": [
        "Once we have text embeddings, we can use them to explore connections in meaning between different words, including calculating similarity between words and completing analogies.\n",
        "\n",
        "We'll start by creating a dictionary containing the vectors for all the words in our vocabulary. We'll stick to the vocabulary above of 800 words from the Yelp reviews - if you want to use more words, change that number! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHlGBkBKoUPI"
      },
      "source": [
        "vocab_dict = dict() #initialize dictionary\n",
        "\n",
        "for word in bow_transformer.vocabulary_:\n",
        "    vocab_dict[word] = nlp(word).vector #what is the key? what is the value?\n",
        "\n",
        "for word, vec in vocab_dict.items(): #iterating through the dictionary to print each key and value\n",
        "  print ('Word: {}. Vector length: {}'.format(word, len(vec)))\n",
        "\n",
        "print()\n",
        "print ('{} words in our dictionary'.format(len(vocab_dict)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPCcIyTpDuab"
      },
      "source": [
        "Next, let's calculate the similarity between two words, using their Word2Vec representations.\n",
        "\n",
        "A common way to calculate the similarity between two vectors is called *cosine similarity*. It depends on the angle between those two vectors when plotted in space. As an example, imagine we had two three-dimensional vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omAmAv88GZUp"
      },
      "source": [
        "v0 = [2,3,1]\n",
        "v1 = [-2,-3,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owbBQZUgGgjs"
      },
      "source": [
        "Run the code below to plot those vectors, and try changing the numbers above.\n",
        "How can you make a very small angle between the vectors? How can you make a very large angle?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtbbBLgcFmE0",
        "cellView": "form"
      },
      "source": [
        "#@title Run this to create an interactive 3D plot\n",
        "#Code from https://stackoverflow.com/questions/47319238/python-plot-3d-vectors \n",
        "import numpy as np \n",
        "import plotly.graph_objs as go\n",
        "\n",
        "def vector_plot(tvects,is_vect=True,orig=[0,0,0]):\n",
        "    \"\"\"Plot vectors using plotly\"\"\"\n",
        "\n",
        "    if is_vect:\n",
        "        if not hasattr(orig[0],\"__iter__\"):\n",
        "            coords = [[orig,np.sum([orig,v],axis=0)] for v in tvects]\n",
        "        else:\n",
        "            coords = [[o,np.sum([o,v],axis=0)] for o,v in zip(orig,tvects)]\n",
        "    else:\n",
        "        coords = tvects\n",
        "\n",
        "    data = []\n",
        "    for i,c in enumerate(coords):\n",
        "        X1, Y1, Z1 = zip(c[0])\n",
        "        X2, Y2, Z2 = zip(c[1])\n",
        "        vector = go.Scatter3d(x = [X1[0],X2[0]],\n",
        "                              y = [Y1[0],Y2[0]],\n",
        "                              z = [Z1[0],Z2[0]],\n",
        "                              marker = dict(size = [0,5],\n",
        "                                            color = ['blue'],\n",
        "                                            line=dict(width=5,\n",
        "                                                      color='DarkSlateGrey')),\n",
        "                              name = 'Vector'+str(i+1))\n",
        "        data.append(vector)\n",
        "\n",
        "    layout = go.Layout(\n",
        "             margin = dict(l = 4,\n",
        "                           r = 4,\n",
        "                           b = 4,\n",
        "                           t = 4)\n",
        "                  )\n",
        "    fig = go.Figure(data=data,layout=layout)\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "vector_plot([v0,v1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7xDNfU3HUE3"
      },
      "source": [
        "For our Word2Vec vectors, we can imagine doing the same thing in 300-dimensional space. Of course, it's much harder to plot that! [Here](https://projector.tensorflow.org/) is one representation that you can play around with.\n",
        "\n",
        "Then we find the cosine of the angle between the two vectors to get the similarity. \n",
        "\n",
        "If the vectors are exactly the same, the angle will be 0, so we get a similarity of cos(0) = 1.\n",
        "\n",
        "If the vectors are exactly opposite, the angle will be 180 degrees, so we get a similarity of cos(180) = -1.\n",
        "\n",
        "There's a useful [mathematical trick](https://www.mathsisfun.com/algebra/vectors-dot-product.html) to find the cosine similarity:\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/1d94e5903f7936d3c131e040ef2c51b473dd071d)\n",
        "\n",
        "Where A_1, A_2, ..., A_300 are the elements of the first vector and B_1, B_2, ..., B_300 are the elements of the second vector.\n",
        "\n",
        "Please implement cosine similarity below, and test it out using our 3-dimensional vectors from above. Do the results make sense?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAnA_KdoNqr-"
      },
      "source": [
        "def vector_cosine_similarity(vec1,vec2):\n",
        "  #Assume vec1 and vec2 have the same size \n",
        "\n",
        "  #YOUR CODE HERE\n",
        "  numerator = 0\n",
        "  for i in range(len(vec1)):\n",
        "    numerator += vec1[i]*vec2[i]\n",
        "  mag1 = (sum(elem**2 for elem in vec1))**0.5\n",
        "  mag2 = (sum(elem**2 for elem in vec2))**0.5\n",
        "  similarity = numerator/(mag1*mag2)\n",
        "  return similarity #number between -1 and 1\n",
        "\n",
        "print(vector_cosine_similarity(v0,v1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ2JQmZELItA"
      },
      "source": [
        "Now, use your cosine similarity function to calculate the similarity between two words. Try out a few words from the dataset - what pairs of words can you find that are particularly similar or particularly dissimilar?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwC6EioNJHKR"
      },
      "source": [
        "def word_similarity(word1, word2):\n",
        "  #Should return a similarity between -1 and 1\n",
        "  \n",
        "  try:\n",
        "    vec1 = vocab_dict[word1]\n",
        "    vec2 = vocab_dict[word2]\n",
        "\n",
        "    #TODO: Fill in the return statement here\n",
        "    return vector_cosine_similarity(vec1,vec2)\n",
        "\n",
        "  except KeyError:\n",
        "    print ('Word not in dictionary')\n",
        "\n",
        "print(word_similarity('burger','steak'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8KKXG2BLz7K"
      },
      "source": [
        "Now, we can use our functions above to find the *most* similar words to any particular word. \n",
        "\n",
        "`find_most_similar(start_vec)` should output the top 5 words whose vectors are most similar to start_vec, with their similarities. Please fill it in.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgNpY_wZpG5A"
      },
      "source": [
        "def find_nearest_neighbor(word):\n",
        "  try:\n",
        "    vec = vocab_dict[word]\n",
        "    find_most_similar(vec)\n",
        "  except KeyError:\n",
        "    print ('Word not in dictionary')\n",
        "\n",
        "def find_most_similar(start_vec):\n",
        "  #Should print the top 5 most similar words to start_vec, and their similarities.,\n",
        "  #Hint: use a for loop to iterate through vocab_dict.\n",
        "  #Consider using a Pandas series.\n",
        "\n",
        "  #YOUR CODE HERE\n",
        "  similarity_series = pd.Series(np.nan, index = vocab_dict.keys())\n",
        "  for word, vec in vocab_dict.items():\n",
        "    similarity_series[word] = vector_cosine_similarity(start_vec, vec)\n",
        "  similarity_series = similarity_series[similarity_series.notna()] #get rid of N/A\n",
        "  five_most_similar = similarity_series.sort_values().tail()\n",
        "  print (five_most_similar) #words and similarities\n",
        "\n",
        "find_nearest_neighbor('bagel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxcyY1YZO9u5"
      },
      "source": [
        "Finally, we can use the functions we've built to complete word analogies, like the ones you can try out [here](http://bionlp-www.utu.fi/wv_demo/). For example:\n",
        "\n",
        "*   Guacamole is to Mexican as pasta is to ________,\n",
        "\n",
        "This requires a bit of \"word arithmetic\":\n",
        "let's say A1, A2, and B1 are vectors for three words we know. We're trying to find B2 to complete \n",
        "\n",
        "*   A1 is to A2 as B1 is to B2.\n",
        "\n",
        "Intuitively, this means that the difference between A1 and A2 is the same as the difference between B1 and B2. So we write\n",
        "\n",
        "*   A1 - A2 = B1 - B2\n",
        "\n",
        "**Solve for B2:**\n",
        "\n",
        "*   B2 = ________________\n",
        "\n",
        "Once we know the vector that we \"expect\" for B2, we can use our previous functions to find the word whose representation is closest to that vector. Try it out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfTeKGqYLw_1"
      },
      "source": [
        "def find_analogy(word_a1, word_a2, word_b1):\n",
        "  #Convert the words to vectors a1, a2, b1\n",
        "  #If word_a1:word_a2 as word_b1:word_b2, then \n",
        "  #a1 - a2 = b1 - b2\n",
        "  #So b2 = ...\n",
        "  #Calculate b2, and use your previous functions to find the best candidates for word_b2.\n",
        "\n",
        "  #YOUR CODE HERE\n",
        "  a1_vec = vocab_dict[word_a1]\n",
        "  a2_vec = vocab_dict[word_a2]\n",
        "  b1_vec = vocab_dict[word_b1]\n",
        "  find_most_similar(b1_vec - a1_vec + a2_vec)\n",
        "\n",
        "find_analogy('guacamole','mexican','pasta')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlDcBn9kQxNR"
      },
      "source": [
        "Word arithmetic doesn't always work perfectly - it's pretty tricky to find good examples! Which can you discover?\n",
        "\n",
        "If you're looking for a way to expand further on this exercise, you can try seeing what happens when you use [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance), another common measurement, instead of cosine similarity."
      ]
    }
  ]
}